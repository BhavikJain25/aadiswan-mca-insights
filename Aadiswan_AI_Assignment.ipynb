{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Generate Mock MCA Data (3 days √ó 5 states)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Fix the typo in \"Gujarat\" from the PDF\n",
        "states = [\"Maharashtra\", \"Gujarat\", \"Delhi\", \"Tamil Nadu\", \"Karnataka\"]\n",
        "company_classes = [\"Private\", \"Public\", \"One Person Company\"]\n",
        "statuses = [\"Active\", \"Strike Off\", \"Amalgamated\", \"Dissolved\"]\n",
        "nic_codes = {\n",
        "    \"1010\": \"Manufacturing\",\n",
        "    \"6202\": \"Computer Programming\",\n",
        "    \"6820\": \"Real Estate\",\n",
        "    \"8299\": \"Business Support Services\"\n",
        "}\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_state_data(state, day_offset=0):\n",
        "    # Base number of companies per state\n",
        "    base_n = 100\n",
        "    if day_offset == 0:\n",
        "        n = base_n\n",
        "    else:\n",
        "        # Add 5-10 new companies each day\n",
        "        n = base_n + np.random.randint(5, 11)\n",
        "\n",
        "    base_date = datetime(2025, 10, 15) + timedelta(days=day_offset)  # Start from recent date\n",
        "\n",
        "    data = []\n",
        "    existing_cins = set()\n",
        "\n",
        "    for i in range(n):\n",
        "        # CIN format: L/U + 20 digits (L = Indian, U = Foreign)\n",
        "        prefix = \"L\" if state in [\"Maharashtra\", \"Delhi\", \"Karnataka\"] else \"U\"\n",
        "        cin = prefix + ''.join(np.random.choice(list('0123456789'), 20))\n",
        "        while cin in existing_cins:\n",
        "            cin = prefix + ''.join(np.random.choice(list('0123456789'), 20))\n",
        "        existing_cins.add(cin)\n",
        "\n",
        "        # Simulate changes over days\n",
        "        if day_offset == 0:\n",
        "            status = np.random.choice(statuses, p=[0.85, 0.10, 0.03, 0.02])\n",
        "            auth_cap = np.random.choice([100000, 500000, 1000000, 5000000])\n",
        "        else:\n",
        "            # Slight drift in status and capital\n",
        "            status = np.random.choice(statuses, p=[0.87, 0.08, 0.03, 0.02])\n",
        "            auth_cap = np.random.choice([100000, 500000, 1000000, 5000000, 10000000])\n",
        "\n",
        "        paid_up = min(auth_cap, np.random.choice([50000, 100000, 200000, 500000]))\n",
        "        nic = np.random.choice(list(nic_codes.keys()))\n",
        "\n",
        "        data.append({\n",
        "            \"CIN\": cin,\n",
        "            \"Company_Name\": f\"{state.replace(' ', '')} Innovations Pvt Ltd {i+1}\",\n",
        "            \"Company_Class\": np.random.choice(company_classes),\n",
        "            \"Date_of_Incorporation\": (base_date - timedelta(days=np.random.randint(0, 730))).strftime(\"%d/%m/%Y\"),\n",
        "            \"Authorized_Capital\": auth_cap,\n",
        "            \"Paid_up_Capital\": paid_up,\n",
        "            \"Company_Status\": status,\n",
        "            \"Principal_Business_Activity\": nic,\n",
        "            \"Registered_Office_Address\": f\"Plot {i%50}, {state} Tech Park\",\n",
        "            \"ROC_Code\": f\"ROC-{state[:3].upper()}\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create directory structure\n",
        "os.makedirs(\"mca_data/day1\", exist_ok=True)\n",
        "os.makedirs(\"mca_data/day2\", exist_ok=True)\n",
        "os.makedirs(\"mca_data/day3\", exist_ok=True)\n",
        "\n",
        "# Generate data for each state and day\n",
        "for state in states:\n",
        "    for day in range(3):\n",
        "        df = generate_state_data(state, day)\n",
        "        filename = f\"mca_data/day{day+1}/{state.replace(' ', '_')}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "print(\"‚úÖ Mock MCA data generated for 5 states across 3 days!\")\n",
        "print(\"üìÅ Files saved in: mca_data/day1/, day2/, day3/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He08tTLYCQlu",
        "outputId": "84ea5206-9233-4498-e037-c4a596cc6e0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mock MCA data generated for 5 states across 3 days!\n",
            "üìÅ Files saved in: mca_data/day1/, day2/, day3/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = pd.read_csv(\"mca_data/day1/Maharashtra.csv\")\n",
        "print(\"Sample from Day 1 - Maharashtra:\")\n",
        "print(df_sample.head(2))\n",
        "print(\"\\nShape:\", df_sample.shape)"
      ],
      "metadata": {
        "id": "m7n3XSkLCQgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1968a25-28fb-4ef5-bf5c-4285c2712fee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from Day 1 - Maharashtra:\n",
            "                     CIN                       Company_Name  \\\n",
            "0  L63746926743772541751  Maharashtra Innovations Pvt Ltd 1   \n",
            "1  L80926382426486138198  Maharashtra Innovations Pvt Ltd 2   \n",
            "\n",
            "        Company_Class Date_of_Incorporation  Authorized_Capital  \\\n",
            "0              Public            24/09/2025              100000   \n",
            "1  One Person Company            18/07/2024              500000   \n",
            "\n",
            "   Paid_up_Capital Company_Status  Principal_Business_Activity  \\\n",
            "0            50000         Active                         8299   \n",
            "1           500000         Active                         8299   \n",
            "\n",
            "       Registered_Office_Address ROC_Code  \n",
            "0  Plot 0, Maharashtra Tech Park  ROC-MAH  \n",
            "1  Plot 1, Maharashtra Tech Park  ROC-MAH  \n",
            "\n",
            "Shape: (100, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Data Integration\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "states = [\"Maharashtra\", \"Gujarat\", \"Delhi\", \"Tamil Nadu\", \"Karnataka\"]\n",
        "\n",
        "def integrate_day_data(day_num):\n",
        "    print(f\"üîÑ Integrating Day {day_num} data...\")\n",
        "    all_dfs = []\n",
        "\n",
        "    for state in states:\n",
        "        filename = f\"mca_data/day{day_num}/{state.replace(' ', '_')}.csv\"\n",
        "        if os.path.exists(filename):\n",
        "            df = pd.read_csv(filename)\n",
        "            df[\"STATE\"] = state\n",
        "            all_dfs.append(df)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: {filename} not found!\")\n",
        "\n",
        "    if not all_dfs:\n",
        "        raise FileNotFoundError(f\"No data found for Day {day_num}\")\n",
        "\n",
        "    master = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    # üîß FIX: Standardize column names SAFELY\n",
        "    # Keep 'CIN' as-is; only clean other columns\n",
        "    new_columns = []\n",
        "    for col in master.columns:\n",
        "        if col.strip().upper() == \"CIN\":\n",
        "            new_columns.append(\"CIN\")  # preserve exact name\n",
        "        else:\n",
        "            # Clean others: strip, replace underscores, title case\n",
        "            clean_col = col.strip().replace(\"_\", \" \").title()\n",
        "            new_columns.append(clean_col)\n",
        "    master.columns = new_columns\n",
        "\n",
        "    # Now ensure \"CIN\" exists\n",
        "    if \"CIN\" not in master.columns:\n",
        "        raise KeyError(\"CIN column missing after standardization!\")\n",
        "\n",
        "    # Fix data\n",
        "    master[\"Company Status\"] = master[\"Company Status\"].astype(str).str.strip()\n",
        "    master[\"State\"] = master[\"State\"].astype(str).str.strip()\n",
        "\n",
        "    # Handle nulls\n",
        "    numeric_cols = [\"Authorized Capital\", \"Paid Up Capital\"]\n",
        "    text_cols = [\"Company Name\", \"Registered Office Address\", \"Company Class\"]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in master.columns:\n",
        "            master[col] = pd.to_numeric(master[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    for col in text_cols:\n",
        "        if col in master.columns:\n",
        "            master[col] = master[col].fillna(\"N/A\").astype(str)\n",
        "\n",
        "    # Deduplicate using CIN (now guaranteed to exist)\n",
        "    master = master.drop_duplicates(subset=[\"CIN\"], keep=\"first\")\n",
        "\n",
        "    # Save\n",
        "    output_path = f\"master_day{day_num}.csv\"\n",
        "    master.to_csv(output_path, index=False)\n",
        "    print(f\"‚úÖ Day {day_num} master saved: {output_path} | Rows: {len(master)}\")\n",
        "    return master\n",
        "\n",
        "# Run for all days\n",
        "master_day1 = integrate_day_data(1)\n",
        "master_day2 = integrate_day_data(2)\n",
        "master_day3 = integrate_day_data(3)\n",
        "\n",
        "print(\"\\nüéâ Data integration complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH0C8eNbRP-f",
        "outputId": "75858f54-ed28-4cf8-bb14-8bee56db74a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Integrating Day 1 data...\n",
            "‚úÖ Day 1 master saved: master_day1.csv | Rows: 500\n",
            "üîÑ Integrating Day 2 data...\n",
            "‚úÖ Day 2 master saved: master_day2.csv | Rows: 537\n",
            "üîÑ Integrating Day 3 data...\n",
            "‚úÖ Day 3 master saved: master_day3.csv | Rows: 540\n",
            "\n",
            "üéâ Data integration complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Change Detection Engine\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load master datasets\n",
        "master1 = pd.read_csv(\"master_day1.csv\")\n",
        "master2 = pd.read_csv(\"master_day2.csv\")\n",
        "master3 = pd.read_csv(\"master_day3.csv\")\n",
        "\n",
        "# Ensure CIN is string (critical for comparison)\n",
        "for df in [master1, master2, master3]:\n",
        "    df[\"CIN\"] = df[\"CIN\"].astype(str)\n",
        "\n",
        "def detect_changes(old_df, new_df, change_date_str):\n",
        "    print(f\"üîç Detecting changes up to {change_date_str}...\")\n",
        "    changes = []\n",
        "\n",
        "    # Convert to dict for fast lookup: {CIN: row}\n",
        "    old_dict = old_df.set_index(\"CIN\").to_dict(orient=\"index\")\n",
        "    new_dict = new_df.set_index(\"CIN\").to_dict(orient=\"index\")\n",
        "\n",
        "    old_cins = set(old_dict.keys())\n",
        "    new_cins = set(new_dict.keys())\n",
        "\n",
        "    # 1. New Incorporations\n",
        "    for cin in (new_cins - old_cins):\n",
        "        changes.append({\n",
        "            \"CIN\": cin,\n",
        "            \"Change_Type\": \"New Incorporation\",\n",
        "            \"Field_Changed\": \"\",\n",
        "            \"Old_Value\": \"\",\n",
        "            \"New_Value\": \"\",\n",
        "            \"Date\": change_date_str\n",
        "        })\n",
        "\n",
        "    # 2. Deregistrations / Missing CINs\n",
        "    for cin in (old_cins - new_cins):\n",
        "        changes.append({\n",
        "            \"CIN\": cin,\n",
        "            \"Change_Type\": \"Deregistered/Struck Off\",\n",
        "            \"Field_Changed\": \"\",\n",
        "            \"Old_Value\": \"\",\n",
        "            \"New_Value\": \"\",\n",
        "            \"Date\": change_date_str\n",
        "        })\n",
        "\n",
        "    # 3. Field Updates (only for CINs present in both)\n",
        "    common_cins = old_cins & new_cins\n",
        "    fields_to_check = [\n",
        "        \"Company Status\",\n",
        "        \"Authorized Capital\",\n",
        "        \"Paid Up Capital\",\n",
        "        \"Company Class\",\n",
        "        \"Principal Business Activity\"\n",
        "    ]\n",
        "\n",
        "    for cin in common_cins:\n",
        "        old_row = old_dict[cin]\n",
        "        new_row = new_dict[cin]\n",
        "        for field in fields_to_check:\n",
        "            if field in old_row and field in new_row:\n",
        "                old_val = str(old_row[field]).strip()\n",
        "                new_val = str(new_row[field]).strip()\n",
        "                if old_val != new_val:\n",
        "                    changes.append({\n",
        "                        \"CIN\": cin,\n",
        "                        \"Change_Type\": \"Field Update\",\n",
        "                        \"Field_Changed\": field,\n",
        "                        \"Old_Value\": old_val,\n",
        "                        \"New_Value\": new_val,\n",
        "                        \"Date\": change_date_str\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(changes)\n",
        "\n",
        "# Define dates (use realistic dates based on your data)\n",
        "# From Step 1, we used base date = 2025-10-15 ‚Üí so:\n",
        "# Day1 = 2025-10-15, Day2 = 2025-10-16, Day3 = 2025-10-17\n",
        "date_day2 = \"2025-10-16\"\n",
        "date_day3 = \"2025-10-17\"\n",
        "\n",
        "# Detect changes\n",
        "changes_1_to_2 = detect_changes(master1, master2, date_day2)\n",
        "changes_2_to_3 = detect_changes(master2, master3, date_day3)\n",
        "\n",
        "# Save change logs\n",
        "changes_1_to_2.to_csv(\"changes_day1_to_day2.csv\", index=False)\n",
        "changes_2_to_3.to_csv(\"changes_day2_to_day3.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Changes Day1‚ÜíDay2: {len(changes_1_to_2)} records saved to changes_day1_to_day2.csv\")\n",
        "print(f\"‚úÖ Changes Day2‚ÜíDay3: {len(changes_2_to_3)} records saved to changes_day2_to_day3.csv\")\n",
        "\n",
        "# Combine all changes for enrichment & dashboard\n",
        "all_changes = pd.concat([changes_1_to_2, changes_2_to_3], ignore_index=True)\n",
        "all_changes.to_csv(\"all_changes.csv\", index=False)\n",
        "print(f\"üì¶ All changes combined: {len(all_changes)} records ‚Üí all_changes.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-2UVRK0RcbY",
        "outputId": "d8a3698e-da35-421c-9b43-9a784975c189"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Detecting changes up to 2025-10-16...\n",
            "üîç Detecting changes up to 2025-10-17...\n",
            "‚úÖ Changes Day1‚ÜíDay2: 1037 records saved to changes_day1_to_day2.csv\n",
            "‚úÖ Changes Day2‚ÜíDay3: 1077 records saved to changes_day2_to_day3.csv\n",
            "üì¶ All changes combined: 2114 records ‚Üí all_changes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Web-Based CIN Enrichment (Mocked Implementation)\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Load latest master data (to get COMPANY_NAME, STATE, STATUS, NIC)\n",
        "master_latest = pd.read_csv(\"master_day3.csv\")\n",
        "master_latest[\"CIN\"] = master_latest[\"CIN\"].astype(str)\n",
        "\n",
        "# Load all changes and sample 75 unique CINs\n",
        "all_changes = pd.read_csv(\"all_changes.csv\")\n",
        "changed_cins = all_changes[\"CIN\"].drop_duplicates().sample(n=min(75, len(all_changes)), random_state=42).tolist()\n",
        "\n",
        "# Filter master data for these CINs\n",
        "enrich_subset = master_latest[master_latest[\"CIN\"].isin(changed_cins)].copy()\n",
        "\n",
        "# NIC to Sector mapping (from MCA/NIC standard)\n",
        "nic_to_sector = {\n",
        "    \"1010\": \"Manufacturing\",\n",
        "    \"6202\": \"Computer Programming & IT Services\",\n",
        "    \"6820\": \"Real Estate Activities\",\n",
        "    \"8299\": \"Business Support Services\",\n",
        "    \"4610\": \"Wholesale Trade\",\n",
        "    \"6499\": \"Financial Services\",\n",
        "    \"8530\": \"Higher Education\",\n",
        "    \"9499\": \"Membership Organizations\"\n",
        "}\n",
        "\n",
        "# Fake director names (for realism)\n",
        "fake_directors = [\n",
        "    \"Rajesh Kumar\", \"Priya Sharma\", \"Amit Patel\", \"Sneha Reddy\", \"Vikram Singh\",\n",
        "    \"Anjali Desai\", \"Karthik Nair\", \"Meera Iyer\", \"Sanjay Mehta\", \"Divya Agarwal\"\n",
        "]\n",
        "\n",
        "# Build enriched records in required format\n",
        "enriched_records = []\n",
        "\n",
        "for _, row in enrich_subset.iterrows():\n",
        "    cin = row[\"CIN\"]\n",
        "    company_name = row[\"Company Name\"]\n",
        "    state = row[\"State\"]\n",
        "    status = row[\"Company Status\"]\n",
        "    nic = str(row.get(\"Principal Business Activity\", \"8299\")).strip()\n",
        "\n",
        "    # Map NIC to sector (fallback to \"Other\")\n",
        "    sector = nic_to_sector.get(nic[:4], \"Other Services\")\n",
        "\n",
        "    # Generate fake director\n",
        "    director = random.choice(fake_directors)\n",
        "\n",
        "    # Simulated source URLs (ZaubaCorp style)\n",
        "    zauba_url = f\"https://www.zaubacorp.com/company/{cin}\"\n",
        "\n",
        "    # Add multiple enrichment rows per company (as per format)\n",
        "    enriched_records.extend([\n",
        "        [cin, company_name, state, status, \"ZaubaCorp\", \"SECTOR\", sector],\n",
        "        [cin, company_name, state, status, \"ZaubaCorp\", \"DIRECTOR\", director],\n",
        "        [cin, company_name, state, status, \"ZaubaCorp\", \"SOURCE_URL\", zauba_url]\n",
        "    ])\n",
        "\n",
        "# Create final enriched DataFrame\n",
        "enriched_df = pd.DataFrame(\n",
        "    enriched_records,\n",
        "    columns=[\"CIN\", \"COMPANY_NAME\", \"STATE\", \"STATUS\", \"SOURCE\", \"FIELD\", \"SOURCE_URL\"]\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "enriched_df.to_csv(\"enriched_data.csv\", index=False)\n",
        "print(f\"‚úÖ Enriched data saved: enriched_data.csv\")\n",
        "print(f\"üìä Sample rows:\")\n",
        "print(enriched_df.head(6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCbblRAlRP33",
        "outputId": "37b62044-eaca-40d4-cc2c-d0cbdffe4631"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enriched data saved: enriched_data.csv\n",
            "üìä Sample rows:\n",
            "                     CIN                        COMPANY_NAME        STATE  \\\n",
            "0  L94811591354050294949  Maharashtra Innovations Pvt Ltd 74  Maharashtra   \n",
            "1  L94811591354050294949  Maharashtra Innovations Pvt Ltd 74  Maharashtra   \n",
            "2  L94811591354050294949  Maharashtra Innovations Pvt Ltd 74  Maharashtra   \n",
            "3  L04237641559958770465  Maharashtra Innovations Pvt Ltd 75  Maharashtra   \n",
            "4  L04237641559958770465  Maharashtra Innovations Pvt Ltd 75  Maharashtra   \n",
            "5  L04237641559958770465  Maharashtra Innovations Pvt Ltd 75  Maharashtra   \n",
            "\n",
            "   STATUS     SOURCE       FIELD  \\\n",
            "0  Active  ZaubaCorp      SECTOR   \n",
            "1  Active  ZaubaCorp    DIRECTOR   \n",
            "2  Active  ZaubaCorp  SOURCE_URL   \n",
            "3  Active  ZaubaCorp      SECTOR   \n",
            "4  Active  ZaubaCorp    DIRECTOR   \n",
            "5  Active  ZaubaCorp  SOURCE_URL   \n",
            "\n",
            "                                          SOURCE_URL  \n",
            "0                          Business Support Services  \n",
            "1                                         Amit Patel  \n",
            "2  https://www.zaubacorp.com/company/L94811591354...  \n",
            "3                             Real Estate Activities  \n",
            "4                                         Amit Patel  \n",
            "5  https://www.zaubacorp.com/company/L04237641559...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: AI Summary Generator\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load change logs\n",
        "changes_1_to_2 = pd.read_csv(\"changes_day1_to_day2.csv\")\n",
        "changes_2_to_3 = pd.read_csv(\"changes_day2_to_day3.csv\")\n",
        "\n",
        "def generate_summary(df, date_str):\n",
        "    # Count change types\n",
        "    new_incorp = int((df[\"Change_Type\"] == \"New Incorporation\").sum())\n",
        "    deregistered = int((df[\"Change_Type\"] == \"Deregistered/Struck Off\").sum())\n",
        "    field_updates = int((df[\"Change_Type\"] == \"Field Update\").sum())\n",
        "\n",
        "    summary_text = f\"Daily Summary\\nNew incorporations: {new_incorp}\\nDeregistered: {deregistered}\\nUpdated records: {field_updates}\"\n",
        "\n",
        "    return {\n",
        "        \"date\": date_str,\n",
        "        \"new_incorporations\": new_incorp,\n",
        "        \"deregistered\": deregistered,\n",
        "        \"field_updates\": field_updates,\n",
        "        \"summary\": summary_text\n",
        "    }\n",
        "\n",
        "# Generate summaries\n",
        "summary_day2 = generate_summary(changes_1_to_2, \"2025-10-16\")\n",
        "summary_day3 = generate_summary(changes_2_to_3, \"2025-10-17\")\n",
        "\n",
        "# Combine into one JSON structure\n",
        "daily_summaries = {\n",
        "    \"summaries\": [summary_day2, summary_day3]\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"daily_summary.json\", \"w\") as f:\n",
        "    json.dump(daily_summaries, f, indent=2)\n",
        "\n",
        "# Also save a .txt version (as shown in PDF)\n",
        "with open(\"daily_summary.txt\", \"w\") as f:\n",
        "    f.write(summary_day3[\"summary\"])  # Latest day as example\n",
        "\n",
        "print(\"‚úÖ AI Summaries generated!\")\n",
        "print(\"\\nüìÑ Latest Summary (2025-10-17):\")\n",
        "print(summary_day3[\"summary\"])\n",
        "print(\"\\nüíæ Saved as: daily_summary.json and daily_summary.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeJCitake2BZ",
        "outputId": "46b051b6-1f3f-4bb4-f4b6-ab3749e3723a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AI Summaries generated!\n",
            "\n",
            "üìÑ Latest Summary (2025-10-17):\n",
            "Daily Summary\n",
            "New incorporations: 540\n",
            "Deregistered: 537\n",
            "Updated records: 0\n",
            "\n",
            "üíæ Saved as: daily_summary.json and daily_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Conversational Chatbot (Rule-Based)\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load latest data\n",
        "master_df = pd.read_csv(\"master_day3.csv\")\n",
        "master_df[\"CIN\"] = master_df[\"CIN\"].astype(str)\n",
        "master_df[\"Authorized Capital\"] = pd.to_numeric(master_df[\"Authorized Capital\"], errors='coerce')\n",
        "\n",
        "all_changes = pd.read_csv(\"all_changes.csv\")\n",
        "all_changes[\"CIN\"] = all_changes[\"CIN\"].astype(str)\n",
        "\n",
        "def chatbot_response(query):\n",
        "    query = query.lower().strip()\n",
        "    print(f\"\\nüí¨ You asked: '{query}'\")\n",
        "\n",
        "    # 1. New incorporations in [STATE]\n",
        "    if \"new incorporation\" in query or (\"new\" in query and \"incorporation\" in query):\n",
        "        if \"maharashtra\" in query:\n",
        "            state_filter = \"Maharashtra\"\n",
        "        elif \"gujarat\" in query:\n",
        "            state_filter = \"Gujarat\"\n",
        "        elif \"delhi\" in query:\n",
        "            state_filter = \"Delhi\"\n",
        "        elif \"tamil\" in query or \"nadu\" in query:\n",
        "            state_filter = \"Tamil Nadu\"\n",
        "        elif \"karnataka\" in query:\n",
        "            state_filter = \"Karnataka\"\n",
        "        else:\n",
        "            state_filter = None\n",
        "\n",
        "        new_cins = all_changes[all_changes[\"Change_Type\"] == \"New Incorporation\"][\"CIN\"].tolist()\n",
        "        if state_filter:\n",
        "            filtered = master_df[\n",
        "                (master_df[\"CIN\"].isin(new_cins)) &\n",
        "                (master_df[\"State\"] == state_filter)\n",
        "            ][[\"CIN\", \"Company Name\"]]\n",
        "            if not filtered.empty:\n",
        "                result = f\"‚úÖ Found {len(filtered)} new incorporations in {state_filter}:\\n\"\n",
        "                for _, row in filtered.head(5).iterrows():\n",
        "                    result += f\"  ‚Ä¢ {row['Company Name']} ({row['CIN']})\\n\"\n",
        "                return result\n",
        "            else:\n",
        "                return f\"‚ùå No new incorporations found in {state_filter}.\"\n",
        "        else:\n",
        "            count = len(new_cins)\n",
        "            return f\"üìä Total new incorporations: {count}\"\n",
        "\n",
        "    # 2. Struck off / Deregistered count\n",
        "    elif \"struck off\" in query or \"deregister\" in query or \"strike off\" in query:\n",
        "        count = len(all_changes[all_changes[\"Change_Type\"] == \"Deregistered/Struck Off\"])\n",
        "        return f\"üóëÔ∏è Companies struck off/deregistered: {count}\"\n",
        "\n",
        "    # 3. Companies with capital above X\n",
        "    elif \"capital\" in query and (\"above\" in query or \"greater\" in query):\n",
        "        # Extract number (e.g., \"10 lakh\" ‚Üí 1000000)\n",
        "        lakh_match = re.search(r'(\\d+)\\s*lakh', query)\n",
        "        if lakh_match:\n",
        "            lakh_val = int(lakh_match.group(1))\n",
        "            min_cap = lakh_val * 100000  # 1 lakh = 100,000\n",
        "        else:\n",
        "            # Try raw number (assume it's in INR)\n",
        "            num_match = re.search(r'(\\d{5,})', query)\n",
        "            if num_match:\n",
        "                min_cap = int(num_match.group(1))\n",
        "            else:\n",
        "                min_cap = 1000000  # default: 10 lakh\n",
        "\n",
        "        high_cap = master_df[master_df[\"Authorized Capital\"] > min_cap][[\"Company Name\", \"Authorized Capital\"]]\n",
        "        if not high_cap.empty:\n",
        "            result = f\"üí∞ Found {len(high_cap)} companies with capital > ‚Çπ{min_cap:,}:\\n\"\n",
        "            for _, row in high_cap.head(5).iterrows():\n",
        "                result += f\"  ‚Ä¢ {row['Company Name']} (‚Çπ{int(row['Authorized Capital']):,})\\n\"\n",
        "            return result\n",
        "        else:\n",
        "            return f\"‚ùå No companies found with capital > ‚Çπ{min_cap:,}.\"\n",
        "\n",
        "    # 4. Default fallback\n",
        "    else:\n",
        "        return (\n",
        "            \"ü§ñ I understand queries like:\\n\"\n",
        "            \"‚Ä¢ 'Show new incorporations in Maharashtra'\\n\"\n",
        "            \"‚Ä¢ 'How many companies were struck off?'\\n\"\n",
        "            \"‚Ä¢ 'List companies with capital above 10 lakh'\\n\"\n",
        "            \"Try one of these!\"\n",
        "        )\n",
        "\n",
        "# üîç Test the chatbot with example queries\n",
        "test_queries = [\n",
        "    \"Show new incorporations in Maharashtra.\",\n",
        "    \"How many companies were struck off?\",\n",
        "    \"List companies with capital above 10 lakh.\"\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    response = chatbot_response(q)\n",
        "    print(response)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1Uawj4e19p",
        "outputId": "af4a25cd-2591-40e7-b98c-fb4194218adc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí¨ You asked: 'show new incorporations in maharashtra.'\n",
            "‚úÖ Found 106 new incorporations in Maharashtra:\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 1 (L57533537803315264347)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 2 (L60851436704202567324)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 3 (L87603665759232094001)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 4 (L24338632525853452737)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 5 (L83726269951253509027)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üí¨ You asked: 'how many companies were struck off?'\n",
            "üóëÔ∏è Companies struck off/deregistered: 1037\n",
            "--------------------------------------------------\n",
            "\n",
            "üí¨ You asked: 'list companies with capital above 10 lakh.'\n",
            "üí∞ Found 232 companies with capital > ‚Çπ1,000,000:\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 1 (‚Çπ5,000,000)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 3 (‚Çπ10,000,000)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 5 (‚Çπ5,000,000)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 9 (‚Çπ10,000,000)\n",
            "  ‚Ä¢ Maharashtra Innovations Pvt Ltd 10 (‚Çπ10,000,000)\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UTxFt6HgWWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Streamlit Dashboard ‚Äî Save as app.py\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# ----------------------------\n",
        "# Load Data\n",
        "# ----------------------------\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    master = pd.read_csv(\"master_day3.csv\")\n",
        "    changes = pd.read_csv(\"all_changes.csv\")\n",
        "    enriched = pd.read_csv(\"enriched_data.csv\")\n",
        "    with open(\"daily_summary.json\", \"r\") as f:\n",
        "        summaries = json.load(f)\n",
        "    return master, changes, enriched, summaries\n",
        "\n",
        "master_df, all_changes, enriched_df, daily_summaries = load_data()\n",
        "\n",
        "# Ensure CIN is string\n",
        "master_df[\"CIN\"] = master_df[\"CIN\"].astype(str)\n",
        "all_changes[\"CIN\"] = all_changes[\"CIN\"].astype(str)\n",
        "\n",
        "# ----------------------------\n",
        "# Chatbot Logic (from Step 6)\n",
        "# ----------------------------\n",
        "def chatbot_response(query):\n",
        "    query = query.lower().strip()\n",
        "    if \"new incorporation\" in query or (\"new\" in query and \"incorporation\" in query):\n",
        "        state_map = {\n",
        "            \"maharashtra\": \"Maharashtra\",\n",
        "            \"gujarat\": \"Gujarat\",\n",
        "            \"delhi\": \"Delhi\",\n",
        "            \"tamil\": \"Tamil Nadu\",\n",
        "            \"nadu\": \"Tamil Nadu\",\n",
        "            \"karnataka\": \"Karnataka\"\n",
        "        }\n",
        "        state_filter = None\n",
        "        for kw, state in state_map.items():\n",
        "            if kw in query:\n",
        "                state_filter = state\n",
        "                break\n",
        "\n",
        "        new_cins = all_changes[all_changes[\"Change_Type\"] == \"New Incorporation\"][\"CIN\"].tolist()\n",
        "        if state_filter:\n",
        "            filtered = master_df[\n",
        "                (master_df[\"CIN\"].isin(new_cins)) &\n",
        "                (master_df[\"State\"] == state_filter)\n",
        "            ][[\"CIN\", \"Company Name\"]]\n",
        "            if not filtered.empty:\n",
        "                result = f\"‚úÖ Found {len(filtered)} new incorporations in {state_filter}:\\n\\n\"\n",
        "                for _, row in filtered.head(5).iterrows():\n",
        "                    result += f\"- **{row['Company Name']}** (`{row['CIN']}`)\\n\"\n",
        "                return result\n",
        "            else:\n",
        "                return f\"‚ùå No new incorporations found in {state_filter}.\"\n",
        "        else:\n",
        "            count = len(new_cins)\n",
        "            return f\"üìä Total new incorporations: **{count}**\"\n",
        "\n",
        "    elif \"struck off\" in query or \"deregister\" in query or \"strike off\" in query:\n",
        "        count = len(all_changes[all_changes[\"Change_Type\"] == \"Deregistered/Struck Off\"])\n",
        "        return f\"üóëÔ∏è Companies struck off/deregistered: **{count}**\"\n",
        "\n",
        "    elif \"capital\" in query and (\"above\" in query or \"greater\" in query):\n",
        "        lakh_match = re.search(r'(\\d+)\\s*lakh', query)\n",
        "        if lakh_match:\n",
        "            min_cap = int(lakh_match.group(1)) * 100000\n",
        "        else:\n",
        "            num_match = re.search(r'(\\d{5,})', query)\n",
        "            min_cap = int(num_match.group(1)) if num_match else 1000000\n",
        "\n",
        "        high_cap = master_df[master_df[\"Authorized Capital\"] > min_cap][[\"Company Name\", \"Authorized Capital\"]]\n",
        "        if not high_cap.empty:\n",
        "            result = f\"üí∞ Found {len(high_cap)} companies with capital > ‚Çπ{min_cap:,}:\\n\\n\"\n",
        "            for _, row in high_cap.head(5).iterrows():\n",
        "                result += f\"- **{row['Company Name']}** (‚Çπ{int(row['Authorized Capital']):,})\\n\"\n",
        "            return result\n",
        "        else:\n",
        "            return f\"‚ùå No companies found with capital > ‚Çπ{min_cap:,}.\"\n",
        "\n",
        "    else:\n",
        "        return (\n",
        "            \"ü§ñ Try these examples:\\n\\n\"\n",
        "            \"- *Show new incorporations in Maharashtra*\\n\"\n",
        "            \"- *How many companies were struck off?*\\n\"\n",
        "            \"- *List companies with capital above 10 lakh*\"\n",
        "        )\n",
        "\n",
        "# ----------------------------\n",
        "# Streamlit App\n",
        "# ----------------------------\n",
        "st.set_page_config(page_title=\"MCA Insights Engine\", layout=\"wide\")\n",
        "st.title(\"üîç MCA Insights Engine\")\n",
        "st.markdown(\"Track company changes across Maharashtra, Gujarat, Delhi, Tamil Nadu & Karnataka\")\n",
        "\n",
        "# Sidebar Filters\n",
        "st.sidebar.header(\"Filters\")\n",
        "states = [\"All\"] + sorted(master_df[\"State\"].dropna().unique().tolist())\n",
        "selected_state = st.sidebar.selectbox(\"State\", states)\n",
        "statuses = [\"All\"] + sorted(master_df[\"Company Status\"].dropna().unique().tolist())\n",
        "selected_status = st.sidebar.selectbox(\"Company Status\", statuses)\n",
        "\n",
        "# Search\n",
        "search_query = st.text_input(\"üîç Search by CIN or Company Name\")\n",
        "\n",
        "# Filter data\n",
        "filtered_df = master_df.copy()\n",
        "if selected_state != \"All\":\n",
        "    filtered_df = filtered_df[filtered_df[\"State\"] == selected_state]\n",
        "if selected_status != \"All\":\n",
        "    filtered_df = filtered_df[filtered_df[\"Company Status\"] == selected_status]\n",
        "if search_query:\n",
        "    filtered_df = filtered_df[\n",
        "        filtered_df[\"CIN\"].str.contains(search_query, case=False, na=False) |\n",
        "        filtered_df[\"Company Name\"].str.contains(search_query, case=False, na=False)\n",
        "    ]\n",
        "\n",
        "# Tabs\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"üìä Dashboard\", \"üìà Change History\", \"üß© Enriched Data\", \"üí¨ Chat with Data\"])\n",
        "\n",
        "# Tab 1: Dashboard\n",
        "with tab1:\n",
        "    st.subheader(\"Company Records\")\n",
        "    st.dataframe(filtered_df[[\n",
        "        \"CIN\", \"Company Name\", \"State\", \"Company Status\",\n",
        "        \"Authorized Capital\", \"Paid Up Capital\", \"Principal Business Activity\"\n",
        "    ]].head(20), use_container_width=True)\n",
        "\n",
        "# Tab 2: Change History\n",
        "with tab2:\n",
        "    st.subheader(\"Daily Change Trends\")\n",
        "    change_counts = all_changes.groupby([\"Date\", \"Change_Type\"]).size().unstack(fill_value=0)\n",
        "    st.bar_chart(change_counts)\n",
        "\n",
        "# Tab 3: Enriched Data\n",
        "with tab3:\n",
        "    st.subheader(\"Enriched Company Info (Sample)\")\n",
        "    enriched_sample = enriched_df.head(15)\n",
        "    st.dataframe(enriched_sample, use_container_width=True)\n",
        "\n",
        "# Tab 4: Chatbot\n",
        "with tab4:\n",
        "    st.subheader(\"Ask Questions in Natural Language\")\n",
        "    user_input = st.text_input(\"üí¨ Type your query below:\")\n",
        "    if user_input:\n",
        "        response = chatbot_response(user_input)\n",
        "        st.markdown(response)\n",
        "\n",
        "# Show latest AI Summary\n",
        "latest_summary = daily_summaries[\"summaries\"][-1][\"summary\"]\n",
        "st.sidebar.markdown(\"### üì∞ Latest AI Summary\")\n",
        "st.sidebar.text(latest_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVDcGq6JgWTK",
        "outputId": "74942119-32fc-4888-9b02-3d5ef7c712e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf\n",
        "conf.get_default().auth_token = \"34IJN7cO4lzAz9UnrJudvvIY0NJ_2cXe5jecf1AD3K7itViyH\"  # ‚Üê paste your token"
      ],
      "metadata": {
        "id": "JiFqiG6YiBya"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected Colab cell to launch Streamlit + ngrok\n",
        "!pip install streamlit pyngrok -q\n",
        "!streamlit run app.py &>/dev/null &\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Optional: If you HAVE an ngrok authtoken, uncomment and add it:\n",
        "# ngrok.set_auth_token(\"your_token_here\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"üîó Dashboard URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvVCMYIRjCYw",
        "outputId": "718401e6-c763-494f-96d2-a525abb7eca9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Dashboard URL: NgrokTunnel: \"https://tiffiny-unshunnable-jorge.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://tiffiny-unshunnable-jorge.ngrok-free.dev/"
      ],
      "metadata": {
        "id": "XrM7aSS_eCap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}