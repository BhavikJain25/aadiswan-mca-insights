{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Generate Mock MCA Data (3 days Ã— 5 states)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Fix the typo in \"Gujarat\" from the PDF\n",
        "states = [\"Maharashtra\", \"Gujarat\", \"Delhi\", \"Tamil Nadu\", \"Karnataka\"]\n",
        "company_classes = [\"Private\", \"Public\", \"One Person Company\"]\n",
        "statuses = [\"Active\", \"Strike Off\", \"Amalgamated\", \"Dissolved\"]\n",
        "nic_codes = {\n",
        "    \"1010\": \"Manufacturing\",\n",
        "    \"6202\": \"Computer Programming\",\n",
        "    \"6820\": \"Real Estate\",\n",
        "    \"8299\": \"Business Support Services\"\n",
        "}\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_state_data(state, day_offset=0):\n",
        "    # Base number of companies per state\n",
        "    base_n = 100\n",
        "    if day_offset == 0:\n",
        "        n = base_n\n",
        "    else:\n",
        "        # Add 5-10 new companies each day\n",
        "        n = base_n + np.random.randint(5, 11)\n",
        "\n",
        "    base_date = datetime(2025, 10, 15) + timedelta(days=day_offset)  # Start from recent date\n",
        "\n",
        "    data = []\n",
        "    existing_cins = set()\n",
        "\n",
        "    for i in range(n):\n",
        "        # CIN format: L/U + 20 digits (L = Indian, U = Foreign)\n",
        "        prefix = \"L\" if state in [\"Maharashtra\", \"Delhi\", \"Karnataka\"] else \"U\"\n",
        "        cin = prefix + ''.join(np.random.choice(list('0123456789'), 20))\n",
        "        while cin in existing_cins:\n",
        "            cin = prefix + ''.join(np.random.choice(list('0123456789'), 20))\n",
        "        existing_cins.add(cin)\n",
        "\n",
        "        # Simulate changes over days\n",
        "        if day_offset == 0:\n",
        "            status = np.random.choice(statuses, p=[0.85, 0.10, 0.03, 0.02])\n",
        "            auth_cap = np.random.choice([100000, 500000, 1000000, 5000000])\n",
        "        else:\n",
        "            # Slight drift in status and capital\n",
        "            status = np.random.choice(statuses, p=[0.87, 0.08, 0.03, 0.02])\n",
        "            auth_cap = np.random.choice([100000, 500000, 1000000, 5000000, 10000000])\n",
        "\n",
        "        paid_up = min(auth_cap, np.random.choice([50000, 100000, 200000, 500000]))\n",
        "        nic = np.random.choice(list(nic_codes.keys()))\n",
        "\n",
        "        data.append({\n",
        "            \"CIN\": cin,\n",
        "            \"Company_Name\": f\"{state.replace(' ', '')} Innovations Pvt Ltd {i+1}\",\n",
        "            \"Company_Class\": np.random.choice(company_classes),\n",
        "            \"Date_of_Incorporation\": (base_date - timedelta(days=np.random.randint(0, 730))).strftime(\"%d/%m/%Y\"),\n",
        "            \"Authorized_Capital\": auth_cap,\n",
        "            \"Paid_up_Capital\": paid_up,\n",
        "            \"Company_Status\": status,\n",
        "            \"Principal_Business_Activity\": nic,\n",
        "            \"Registered_Office_Address\": f\"Plot {i%50}, {state} Tech Park\",\n",
        "            \"ROC_Code\": f\"ROC-{state[:3].upper()}\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create directory structure\n",
        "os.makedirs(\"mca_data/day1\", exist_ok=True)\n",
        "os.makedirs(\"mca_data/day2\", exist_ok=True)\n",
        "os.makedirs(\"mca_data/day3\", exist_ok=True)\n",
        "\n",
        "# Generate data for each state and day\n",
        "for state in states:\n",
        "    for day in range(3):\n",
        "        df = generate_state_data(state, day)\n",
        "        filename = f\"mca_data/day{day+1}/{state.replace(' ', '_')}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "print(\"âœ… Mock MCA data generated for 5 states across 3 days!\")\n",
        "print(\"ðŸ“ Files saved in: mca_data/day1/, day2/, day3/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He08tTLYCQlu",
        "outputId": "84ea5206-9233-4498-e037-c4a596cc6e0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Mock MCA data generated for 5 states across 3 days!\n",
            "ðŸ“ Files saved in: mca_data/day1/, day2/, day3/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = pd.read_csv(\"mca_data/day1/Maharashtra.csv\")\n",
        "print(\"Sample from Day 1 - Maharashtra:\")\n",
        "print(df_sample.head(2))\n",
        "print(\"\\nShape:\", df_sample.shape)"
      ],
      "metadata": {
        "id": "m7n3XSkLCQgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1968a25-28fb-4ef5-bf5c-4285c2712fee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from Day 1 - Maharashtra:\n",
            "                     CIN                       Company_Name  \\\n",
            "0  L63746926743772541751  Maharashtra Innovations Pvt Ltd 1   \n",
            "1  L80926382426486138198  Maharashtra Innovations Pvt Ltd 2   \n",
            "\n",
            "        Company_Class Date_of_Incorporation  Authorized_Capital  \\\n",
            "0              Public            24/09/2025              100000   \n",
            "1  One Person Company            18/07/2024              500000   \n",
            "\n",
            "   Paid_up_Capital Company_Status  Principal_Business_Activity  \\\n",
            "0            50000         Active                         8299   \n",
            "1           500000         Active                         8299   \n",
            "\n",
            "       Registered_Office_Address ROC_Code  \n",
            "0  Plot 0, Maharashtra Tech Park  ROC-MAH  \n",
            "1  Plot 1, Maharashtra Tech Park  ROC-MAH  \n",
            "\n",
            "Shape: (100, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Data Integration\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "states = [\"Maharashtra\", \"Gujarat\", \"Delhi\", \"Tamil Nadu\", \"Karnataka\"]\n",
        "\n",
        "def integrate_day_data(day_num):\n",
        "    print(f\"ðŸ”„ Integrating Day {day_num} data...\")\n",
        "    all_dfs = []\n",
        "\n",
        "    for state in states:\n",
        "        filename = f\"mca_data/day{day_num}/{state.replace(' ', '_')}.csv\"\n",
        "        if os.path.exists(filename):\n",
        "            df = pd.read_csv(filename)\n",
        "            df[\"STATE\"] = state\n",
        "            all_dfs.append(df)\n",
        "        else:\n",
        "            print(f\"âš ï¸ Warning: {filename} not found!\")\n",
        "\n",
        "    if not all_dfs:\n",
        "        raise FileNotFoundError(f\"No data found for Day {day_num}\")\n",
        "\n",
        "    master = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    # ðŸ”§ FIX: Standardize column names SAFELY\n",
        "    # Keep 'CIN' as-is; only clean other columns\n",
        "    new_columns = []\n",
        "    for col in master.columns:\n",
        "        if col.strip().upper() == \"CIN\":\n",
        "            new_columns.append(\"CIN\")  # preserve exact name\n",
        "        else:\n",
        "            # Clean others: strip, replace underscores, title case\n",
        "            clean_col = col.strip().replace(\"_\", \" \").title()\n",
        "            new_columns.append(clean_col)\n",
        "    master.columns = new_columns\n",
        "\n",
        "    # Now ensure \"CIN\" exists\n",
        "    if \"CIN\" not in master.columns:\n",
        "        raise KeyError(\"CIN column missing after standardization!\")\n",
        "\n",
        "    # Fix data\n",
        "    master[\"Company Status\"] = master[\"Company Status\"].astype(str).str.strip()\n",
        "    master[\"State\"] = master[\"State\"].astype(str).str.strip()\n",
        "\n",
        "    # Handle nulls\n",
        "    numeric_cols = [\"Authorized Capital\", \"Paid Up Capital\"]\n",
        "    text_cols = [\"Company Name\", \"Registered Office Address\", \"Company Class\"]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in master.columns:\n",
        "            master[col] = pd.to_numeric(master[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    for col in text_cols:\n",
        "        if col in master.columns:\n",
        "            master[col] = master[col].fillna(\"N/A\").astype(str)\n",
        "\n",
        "    # Deduplicate using CIN (now guaranteed to exist)\n",
        "    master = master.drop_duplicates(subset=[\"CIN\"], keep=\"first\")\n",
        "\n",
        "    # Save\n",
        "    output_path = f\"master_day{day_num}.csv\"\n",
        "    master.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… Day {day_num} master saved: {output_path} | Rows: {len(master)}\")\n",
        "    return master\n",
        "\n",
        "# Run for all days\n",
        "master_day1 = integrate_day_data(1)\n",
        "master_day2 = integrate_day_data(2)\n",
        "master_day3 = integrate_day_data(3)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Data integration complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH0C8eNbRP-f",
        "outputId": "75858f54-ed28-4cf8-bb14-8bee56db74a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Integrating Day 1 data...\n",
            "âœ… Day 1 master saved: master_day1.csv | Rows: 500\n",
            "ðŸ”„ Integrating Day 2 data...\n",
            "âœ… Day 2 master saved: master_day2.csv | Rows: 537\n",
            "ðŸ”„ Integrating Day 3 data...\n",
            "âœ… Day 3 master saved: master_day3.csv | Rows: 540\n",
            "\n",
            "ðŸŽ‰ Data integration complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Change Detection Engine\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load master datasets\n",
        "master1 = pd.read_csv(\"master_day1.csv\")\n",
        "master2 = pd.read_csv(\"master_day2.csv\")\n",
        "master3 = pd.read_csv(\"master_day3.csv\")\n",
        "\n",
        "# Ensure CIN is string (critical for comparison)\n",
        "for df in [master1, master2, master3]:\n",
        "    df[\"CIN\"] = df[\"CIN\"].astype(str)\n",
        "\n",
        "def detect_changes(old_df, new_df, change_date_str):\n",
        "    print(f\"ðŸ” Detecting changes up to {change_date_str}...\")\n",
        "    changes = []\n",
        "\n",
        "    # Convert to dict for fast lookup: {CIN: row}\n",
        "    old_dict = old_df.set_index(\"CIN\").to_dict(orient=\"index\")\n",
        "    new_dict = new_df.set_index(\"CIN\").to_dict(orient=\"index\")\n",
        "\n",
        "    old_cins = set(old_dict.keys())\n",
        "    new_cins = set(new_dict.keys())\n",
        "\n",
        "    # 1. New Incorporations\n",
        "    for cin in (new_cins - old_cins):\n",
        "        changes.append({\n",
        "            \"CIN\": cin,\n",
        "            \"Change_Type\": \"New Incorporation\",\n",
        "            \"Field_Changed\": \"\",\n",
        "            \"Old_Value\": \"\",\n",
        "            \"New_Value\": \"\",\n",
        "            \"Date\": change_date_str\n",
        "        })\n",
        "\n",
        "    # 2. Deregistrations / Missing CINs\n",
        "    for cin in (old_cins - new_cins):\n",
        "        changes.append({\n",
        "            \"CIN\": cin,\n",
        "            \"Change_Type\": \"Deregistered/Struck Off\",\n",
        "            \"Field_Changed\": \"\",\n",
        "            \"Old_Value\": \"\",\n",
        "            \"New_Value\": \"\",\n",
        "            \"Date\": change_date_str\n",
        "        })\n",
        "\n",
        "    # 3. Field Updates (only for CINs present in both)\n",
        "    common_cins = old_cins & new_cins\n",
        "    fields_to_check = [\n",
        "        \"Company Status\",\n",
        "        \"Authorized Capital\",\n",
        "        \"Paid Up Capital\",\n",
        "        \"Company Class\",\n",
        "        \"Principal Business Activity\"\n",
        "    ]\n",
        "\n",
        "    for cin in common_cins:\n",
        "        old_row = old_dict[cin]\n",
        "        new_row = new_dict[cin]\n",
        "        for field in fields_to_check:\n",
        "            if field in old_row and field in new_row:\n",
        "                old_val = str(old_row[field]).strip()\n",
        "                new_val = str(new_row[field]).strip()\n",
        "                if old_val != new_val:\n",
        "                    changes.append({\n",
        "                        \"CIN\": cin,\n",
        "                        \"Change_Type\": \"Field Update\",\n",
        "                        \"Field_Changed\": field,\n",
        "                        \"Old_Value\": old_val,\n",
        "                        \"New_Value\": new_val,\n",
        "                        \"Date\": change_date_str\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(changes)\n",
        "\n",
        "# Define dates (use realistic dates based on your data)\n",
        "# From Step 1, we used base date = 2025-10-15 â†’ so:\n",
        "# Day1 = 2025-10-15, Day2 = 2025-10-16, Day3 = 2025-10-17\n",
        "date_day2 = \"2025-10-16\"\n",
        "date_day3 = \"2025-10-17\"\n",
        "\n",
        "# Detect changes\n",
        "changes_1_to_2 = detect_changes(master1, master2, date_day2)\n",
        "changes_2_to_3 = detect_changes(master2, master3, date_day3)\n",
        "\n",
        "# Save change logs\n",
        "changes_1_to_2.to_csv(\"changes_day1_to_day2.csv\", index=False)\n",
        "changes_2_to_3.to_csv(\"changes_day2_to_day3.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Changes Day1â†’Day2: {len(changes_1_to_2)} records saved to changes_day1_to_day2.csv\")\n",
        "print(f\"âœ… Changes Day2â†’Day3: {len(changes_2_to_3)} records saved to changes_day2_to_day3.csv\")\n",
        "\n",
        "# Combine all changes for enrichment & dashboard\n",
        "all_changes = pd.concat([changes_1_to_2, changes_2_to_3], ignore_index=True)\n",
        "all_changes.to_csv(\"all_changes.csv\", index=False)\n",
        "print(f\"ðŸ“¦ All changes combined: {len(all_changes)} records â†’ all_changes.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-2UVRK0RcbY",
        "outputId": "d8a3698e-da35-421c-9b43-9a784975c189"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Detecting changes up to 2025-10-16...\n",
            "ðŸ” Detecting changes up to 2025-10-17...\n",
            "âœ… Changes Day1â†’Day2: 1037 records saved to changes_day1_to_day2.csv\n",
            "âœ… Changes Day2â†’Day3: 1077 records saved to changes_day2_to_day3.csv\n",
            "ðŸ“¦ All changes combined: 2114 records â†’ all_changes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Web-Based CIN Enrichment (Mocked Implementation)\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Load latest master data (to get COMPANY_NAME, STATE, STATUS, NIC)\n",
        "master_latest = pd.read_csv(\"master_day3.csv\")\n",
        "master_latest[\"CIN\"] = master_latest[\"CIN\"].astype(str)\n",
        "\n",
        "# Load all changes and sample 75 unique CINs\n",
        "all_changes = pd.read_csv(\"all_changes.csv\")\n",
        "changed_cins = all_changes[\"CIN\"].drop_duplicates().sample(n=min(75, len(all_changes)), random_state=42).tolist()\n",
        "\n",
        "# Filter master data for these CINs\n",
        "enrich_subset = master_latest[master_latest[\"CIN\"].isin(changed_cins)].copy()\n",
        "\n",
        "# NIC to Sector mapping (from MCA/NIC standard)\n",
        "nic_to_sector = {\n",
        "    \"1010\": \"Manufacturing\",\n",
        "    \"6202\": \"Computer Programming & IT Services\",\n",
        "    \"6820\": \"Real Estate Activities\",\n",
        "    \"8299\": \"Business Support Services\",\n",
        "    \"4610\": \"Wholesale Trade\",\n",
        "    \"6499\": \"Financial Services\",\n",
        "    \"8530\": \"Higher Education\",\n",
        "    \"9499\": \"Membership Organizations\"\n",
        "}\n",
        "\n",
        "# Fake director names (for realism)\n",
        "fake_directors = [\n",
        "    \"Rajesh Kumar\", \"Priya Sharma\", \"Amit Patel\", \"Sneha Reddy\", \"Vikram Singh\",\n",
        "    \"Anjali Desai\", \"Karthik Nair\", \"Meera Iyer\", \"Sanjay Mehta\", \"Divya Agarwal\"\n",
        "]\n",
        "\n",
        "# Build enriched records in required format\n",
        "enriched_records = []\n",
        "\n",
        "for _, row in enrich_subset.iterrows():\n",
        "    cin = row[\"CIN\"]\n",
        "    company_name = row[\"Company Name\"]\n",
        "    state = row[\"State\"]\n",
        "    status = row[\"Company Status\"]\n",
        "    nic = str(row.get(\"Principal Business Activity\", \"8299\")).strip()\n",
        "\n",
        "    # Map NIC to sector (fallback to \"Other\")\n",
        "    sector = nic_to_sector.get(nic[:4], \"Other Services\")\n",
        "\n",
        "    # Generate fake director\n",
        "    director = random.choice(fake_directors)\n",
        "\n",
        "    # Simulated source URLs (ZaubaCorp style)\n",
        "    zauba_url = f\"https://www.zaubacorp.com/company/{cin}\"\n",
        "\n",
        "    # Add multiple enrichment rows per company (as per format)\n",
        "    enriched_records.extend([\n",
        "        [cin, company_name, state, status, \"ZaubaCorp\", \"SECTOR\", sector],\n",
        "        [cin, company_name, state, status, \"ZaubaCorp\", \"DIRECTOR\", director],\n",
        "        [cin, company_name, state, status, \"ZaubaCorp\", \"SOURCE_URL\", zauba_url]\n",
        "    ])\n",
        "\n",
        "# Create final enriched DataFrame\n",
        "enriched_df = pd.DataFrame(\n",
        "    enriched_records,\n",
        "    columns=[\"CIN\", \"COMPANY_NAME\", \"STATE\", \"STATUS\", \"SOURCE\", \"FIELD\", \"SOURCE_URL\"]\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "enriched_df.to_csv(\"enriched_data.csv\", index=False)\n",
        "print(f\"âœ… Enriched data saved: enriched_data.csv\")\n",
        "print(f\"ðŸ“Š Sample rows:\")\n",
        "print(enriched_df.head(6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCbblRAlRP33",
        "outputId": "37b62044-eaca-40d4-cc2c-d0cbdffe4631"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Enriched data saved: enriched_data.csv\n",
            "ðŸ“Š Sample rows:\n",
            "                     CIN                        COMPANY_NAME        STATE  \\\n",
            "0  L94811591354050294949  Maharashtra Innovations Pvt Ltd 74  Maharashtra   \n",
            "1  L94811591354050294949  Maharashtra Innovations Pvt Ltd 74  Maharashtra   \n",
            "2  L94811591354050294949  Maharashtra Innovations Pvt Ltd 74  Maharashtra   \n",
            "3  L04237641559958770465  Maharashtra Innovations Pvt Ltd 75  Maharashtra   \n",
            "4  L04237641559958770465  Maharashtra Innovations Pvt Ltd 75  Maharashtra   \n",
            "5  L04237641559958770465  Maharashtra Innovations Pvt Ltd 75  Maharashtra   \n",
            "\n",
            "   STATUS     SOURCE       FIELD  \\\n",
            "0  Active  ZaubaCorp      SECTOR   \n",
            "1  Active  ZaubaCorp    DIRECTOR   \n",
            "2  Active  ZaubaCorp  SOURCE_URL   \n",
            "3  Active  ZaubaCorp      SECTOR   \n",
            "4  Active  ZaubaCorp    DIRECTOR   \n",
            "5  Active  ZaubaCorp  SOURCE_URL   \n",
            "\n",
            "                                          SOURCE_URL  \n",
            "0                          Business Support Services  \n",
            "1                                         Amit Patel  \n",
            "2  https://www.zaubacorp.com/company/L94811591354...  \n",
            "3                             Real Estate Activities  \n",
            "4                                         Amit Patel  \n",
            "5  https://www.zaubacorp.com/company/L04237641559...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: AI Summary Generator\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load change logs\n",
        "changes_1_to_2 = pd.read_csv(\"changes_day1_to_day2.csv\")\n",
        "changes_2_to_3 = pd.read_csv(\"changes_day2_to_day3.csv\")\n",
        "\n",
        "def generate_summary(df, date_str):\n",
        "    # Count change types\n",
        "    new_incorp = int((df[\"Change_Type\"] == \"New Incorporation\").sum())\n",
        "    deregistered = int((df[\"Change_Type\"] == \"Deregistered/Struck Off\").sum())\n",
        "    field_updates = int((df[\"Change_Type\"] == \"Field Update\").sum())\n",
        "\n",
        "    summary_text = f\"Daily Summary\\nNew incorporations: {new_incorp}\\nDeregistered: {deregistered}\\nUpdated records: {field_updates}\"\n",
        "\n",
        "    return {\n",
        "        \"date\": date_str,\n",
        "        \"new_incorporations\": new_incorp,\n",
        "        \"deregistered\": deregistered,\n",
        "        \"field_updates\": field_updates,\n",
        "        \"summary\": summary_text\n",
        "    }\n",
        "\n",
        "# Generate summaries\n",
        "summary_day2 = generate_summary(changes_1_to_2, \"2025-10-16\")\n",
        "summary_day3 = generate_summary(changes_2_to_3, \"2025-10-17\")\n",
        "\n",
        "# Combine into one JSON structure\n",
        "daily_summaries = {\n",
        "    \"summaries\": [summary_day2, summary_day3]\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"daily_summary.json\", \"w\") as f:\n",
        "    json.dump(daily_summaries, f, indent=2)\n",
        "\n",
        "# Also save a .txt version (as shown in PDF)\n",
        "with open(\"daily_summary.txt\", \"w\") as f:\n",
        "    f.write(summary_day3[\"summary\"])  # Latest day as example\n",
        "\n",
        "print(\"âœ… AI Summaries generated!\")\n",
        "print(\"\\nðŸ“„ Latest Summary (2025-10-17):\")\n",
        "print(summary_day3[\"summary\"])\n",
        "print(\"\\nðŸ’¾ Saved as: daily_summary.json and daily_summary.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeJCitake2BZ",
        "outputId": "46b051b6-1f3f-4bb4-f4b6-ab3749e3723a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… AI Summaries generated!\n",
            "\n",
            "ðŸ“„ Latest Summary (2025-10-17):\n",
            "Daily Summary\n",
            "New incorporations: 540\n",
            "Deregistered: 537\n",
            "Updated records: 0\n",
            "\n",
            "ðŸ’¾ Saved as: daily_summary.json and daily_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Conversational Chatbot (Rule-Based)\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load latest data\n",
        "master_df = pd.read_csv(\"master_day3.csv\")\n",
        "master_df[\"CIN\"] = master_df[\"CIN\"].astype(str)\n",
        "master_df[\"Authorized Capital\"] = pd.to_numeric(master_df[\"Authorized Capital\"], errors='coerce')\n",
        "\n",
        "all_changes = pd.read_csv(\"all_changes.csv\")\n",
        "all_changes[\"CIN\"] = all_changes[\"CIN\"].astype(str)\n",
        "\n",
        "def chatbot_response(query):\n",
        "    query = query.lower().strip()\n",
        "    print(f\"\\nðŸ’¬ You asked: '{query}'\")\n",
        "\n",
        "    # 1. New incorporations in [STATE]\n",
        "    if \"new incorporation\" in query or (\"new\" in query and \"incorporation\" in query):\n",
        "        if \"maharashtra\" in query:\n",
        "            state_filter = \"Maharashtra\"\n",
        "        elif \"gujarat\" in query:\n",
        "            state_filter = \"Gujarat\"\n",
        "        elif \"delhi\" in query:\n",
        "            state_filter = \"Delhi\"\n",
        "        elif \"tamil\" in query or \"nadu\" in query:\n",
        "            state_filter = \"Tamil Nadu\"\n",
        "        elif \"karnataka\" in query:\n",
        "            state_filter = \"Karnataka\"\n",
        "        else:\n",
        "            state_filter = None\n",
        "\n",
        "        new_cins = all_changes[all_changes[\"Change_Type\"] == \"New Incorporation\"][\"CIN\"].tolist()\n",
        "        if state_filter:\n",
        "            filtered = master_df[\n",
        "                (master_df[\"CIN\"].isin(new_cins)) &\n",
        "                (master_df[\"State\"] == state_filter)\n",
        "            ][[\"CIN\", \"Company Name\"]]\n",
        "            if not filtered.empty:\n",
        "                result = f\"âœ… Found {len(filtered)} new incorporations in {state_filter}:\\n\"\n",
        "                for _, row in filtered.head(5).iterrows():\n",
        "                    result += f\"  â€¢ {row['Company Name']} ({row['CIN']})\\n\"\n",
        "                return result\n",
        "            else:\n",
        "                return f\"âŒ No new incorporations found in {state_filter}.\"\n",
        "        else:\n",
        "            count = len(new_cins)\n",
        "            return f\"ðŸ“Š Total new incorporations: {count}\"\n",
        "\n",
        "    # 2. Struck off / Deregistered count\n",
        "    elif \"struck off\" in query or \"deregister\" in query or \"strike off\" in query:\n",
        "        count = len(all_changes[all_changes[\"Change_Type\"] == \"Deregistered/Struck Off\"])\n",
        "        return f\"ðŸ—‘ï¸ Companies struck off/deregistered: {count}\"\n",
        "\n",
        "    # 3. Companies with capital above X\n",
        "    elif \"capital\" in query and (\"above\" in query or \"greater\" in query):\n",
        "        # Extract number (e.g., \"10 lakh\" â†’ 1000000)\n",
        "        lakh_match = re.search(r'(\\d+)\\s*lakh', query)\n",
        "        if lakh_match:\n",
        "            lakh_val = int(lakh_match.group(1))\n",
        "            min_cap = lakh_val * 100000  # 1 lakh = 100,000\n",
        "        else:\n",
        "            # Try raw number (assume it's in INR)\n",
        "            num_match = re.search(r'(\\d{5,})', query)\n",
        "            if num_match:\n",
        "                min_cap = int(num_match.group(1))\n",
        "            else:\n",
        "                min_cap = 1000000  # default: 10 lakh\n",
        "\n",
        "        high_cap = master_df[master_df[\"Authorized Capital\"] > min_cap][[\"Company Name\", \"Authorized Capital\"]]\n",
        "        if not high_cap.empty:\n",
        "            result = f\"ðŸ’° Found {len(high_cap)} companies with capital > â‚¹{min_cap:,}:\\n\"\n",
        "            for _, row in high_cap.head(5).iterrows():\n",
        "                result += f\"  â€¢ {row['Company Name']} (â‚¹{int(row['Authorized Capital']):,})\\n\"\n",
        "            return result\n",
        "        else:\n",
        "            return f\"âŒ No companies found with capital > â‚¹{min_cap:,}.\"\n",
        "\n",
        "    # 4. Default fallback\n",
        "    else:\n",
        "        return (\n",
        "            \"ðŸ¤– I understand queries like:\\n\"\n",
        "            \"â€¢ 'Show new incorporations in Maharashtra'\\n\"\n",
        "            \"â€¢ 'How many companies were struck off?'\\n\"\n",
        "            \"â€¢ 'List companies with capital above 10 lakh'\\n\"\n",
        "            \"Try one of these!\"\n",
        "        )\n",
        "\n",
        "# ðŸ” Test the chatbot with example queries\n",
        "test_queries = [\n",
        "    \"Show new incorporations in Maharashtra.\",\n",
        "    \"How many companies were struck off?\",\n",
        "    \"List companies with capital above 10 lakh.\"\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    response = chatbot_response(q)\n",
        "    print(response)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1Uawj4e19p",
        "outputId": "af4a25cd-2591-40e7-b98c-fb4194218adc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¬ You asked: 'show new incorporations in maharashtra.'\n",
            "âœ… Found 106 new incorporations in Maharashtra:\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 1 (L57533537803315264347)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 2 (L60851436704202567324)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 3 (L87603665759232094001)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 4 (L24338632525853452737)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 5 (L83726269951253509027)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ’¬ You asked: 'how many companies were struck off?'\n",
            "ðŸ—‘ï¸ Companies struck off/deregistered: 1037\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ’¬ You asked: 'list companies with capital above 10 lakh.'\n",
            "ðŸ’° Found 232 companies with capital > â‚¹1,000,000:\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 1 (â‚¹5,000,000)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 3 (â‚¹10,000,000)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 5 (â‚¹5,000,000)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 9 (â‚¹10,000,000)\n",
            "  â€¢ Maharashtra Innovations Pvt Ltd 10 (â‚¹10,000,000)\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UTxFt6HgWWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Streamlit Dashboard â€” Save as app.py\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# ----------------------------\n",
        "# Load Data\n",
        "# ----------------------------\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    master = pd.read_csv(\"master_day3.csv\")\n",
        "    changes = pd.read_csv(\"all_changes.csv\")\n",
        "    enriched = pd.read_csv(\"enriched_data.csv\")\n",
        "    with open(\"daily_summary.json\", \"r\") as f:\n",
        "        summaries = json.load(f)\n",
        "    return master, changes, enriched, summaries\n",
        "\n",
        "master_df, all_changes, enriched_df, daily_summaries = load_data()\n",
        "\n",
        "# Ensure CIN is string\n",
        "master_df[\"CIN\"] = master_df[\"CIN\"].astype(str)\n",
        "all_changes[\"CIN\"] = all_changes[\"CIN\"].astype(str)\n",
        "\n",
        "# ----------------------------\n",
        "# Chatbot Logic (from Step 6)\n",
        "# ----------------------------\n",
        "def chatbot_response(query):\n",
        "    query = query.lower().strip()\n",
        "    if \"new incorporation\" in query or (\"new\" in query and \"incorporation\" in query):\n",
        "        state_map = {\n",
        "            \"maharashtra\": \"Maharashtra\",\n",
        "            \"gujarat\": \"Gujarat\",\n",
        "            \"delhi\": \"Delhi\",\n",
        "            \"tamil\": \"Tamil Nadu\",\n",
        "            \"nadu\": \"Tamil Nadu\",\n",
        "            \"karnataka\": \"Karnataka\"\n",
        "        }\n",
        "        state_filter = None\n",
        "        for kw, state in state_map.items():\n",
        "            if kw in query:\n",
        "                state_filter = state\n",
        "                break\n",
        "\n",
        "        new_cins = all_changes[all_changes[\"Change_Type\"] == \"New Incorporation\"][\"CIN\"].tolist()\n",
        "        if state_filter:\n",
        "            filtered = master_df[\n",
        "                (master_df[\"CIN\"].isin(new_cins)) &\n",
        "                (master_df[\"State\"] == state_filter)\n",
        "            ][[\"CIN\", \"Company Name\"]]\n",
        "            if not filtered.empty:\n",
        "                result = f\"âœ… Found {len(filtered)} new incorporations in {state_filter}:\\n\\n\"\n",
        "                for _, row in filtered.head(5).iterrows():\n",
        "                    result += f\"- **{row['Company Name']}** (`{row['CIN']}`)\\n\"\n",
        "                return result\n",
        "            else:\n",
        "                return f\"âŒ No new incorporations found in {state_filter}.\"\n",
        "        else:\n",
        "            count = len(new_cins)\n",
        "            return f\"ðŸ“Š Total new incorporations: **{count}**\"\n",
        "\n",
        "    elif \"struck off\" in query or \"deregister\" in query or \"strike off\" in query:\n",
        "        count = len(all_changes[all_changes[\"Change_Type\"] == \"Deregistered/Struck Off\"])\n",
        "        return f\"ðŸ—‘ï¸ Companies struck off/deregistered: **{count}**\"\n",
        "\n",
        "    elif \"capital\" in query and (\"above\" in query or \"greater\" in query):\n",
        "        lakh_match = re.search(r'(\\d+)\\s*lakh', query)\n",
        "        if lakh_match:\n",
        "            min_cap = int(lakh_match.group(1)) * 100000\n",
        "        else:\n",
        "            num_match = re.search(r'(\\d{5,})', query)\n",
        "            min_cap = int(num_match.group(1)) if num_match else 1000000\n",
        "\n",
        "        high_cap = master_df[master_df[\"Authorized Capital\"] > min_cap][[\"Company Name\", \"Authorized Capital\"]]\n",
        "        if not high_cap.empty:\n",
        "            result = f\"ðŸ’° Found {len(high_cap)} companies with capital > â‚¹{min_cap:,}:\\n\\n\"\n",
        "            for _, row in high_cap.head(5).iterrows():\n",
        "                result += f\"- **{row['Company Name']}** (â‚¹{int(row['Authorized Capital']):,})\\n\"\n",
        "            return result\n",
        "        else:\n",
        "            return f\"âŒ No companies found with capital > â‚¹{min_cap:,}.\"\n",
        "\n",
        "    else:\n",
        "        return (\n",
        "            \"ðŸ¤– Try these examples:\\n\\n\"\n",
        "            \"- *Show new incorporations in Maharashtra*\\n\"\n",
        "            \"- *How many companies were struck off?*\\n\"\n",
        "            \"- *List companies with capital above 10 lakh*\"\n",
        "        )\n",
        "\n",
        "# ----------------------------\n",
        "# Streamlit App\n",
        "# ----------------------------\n",
        "st.set_page_config(page_title=\"MCA Insights Engine\", layout=\"wide\")\n",
        "st.title(\"ðŸ” MCA Insights Engine\")\n",
        "st.markdown(\"Track company changes across Maharashtra, Gujarat, Delhi, Tamil Nadu & Karnataka\")\n",
        "\n",
        "# Sidebar Filters\n",
        "st.sidebar.header(\"Filters\")\n",
        "states = [\"All\"] + sorted(master_df[\"State\"].dropna().unique().tolist())\n",
        "selected_state = st.sidebar.selectbox(\"State\", states)\n",
        "statuses = [\"All\"] + sorted(master_df[\"Company Status\"].dropna().unique().tolist())\n",
        "selected_status = st.sidebar.selectbox(\"Company Status\", statuses)\n",
        "\n",
        "# Search\n",
        "search_query = st.text_input(\"ðŸ” Search by CIN or Company Name\")\n",
        "\n",
        "# Filter data\n",
        "filtered_df = master_df.copy()\n",
        "if selected_state != \"All\":\n",
        "    filtered_df = filtered_df[filtered_df[\"State\"] == selected_state]\n",
        "if selected_status != \"All\":\n",
        "    filtered_df = filtered_df[filtered_df[\"Company Status\"] == selected_status]\n",
        "if search_query:\n",
        "    filtered_df = filtered_df[\n",
        "        filtered_df[\"CIN\"].str.contains(search_query, case=False, na=False) |\n",
        "        filtered_df[\"Company Name\"].str.contains(search_query, case=False, na=False)\n",
        "    ]\n",
        "\n",
        "# Tabs\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"ðŸ“Š Dashboard\", \"ðŸ“ˆ Change History\", \"ðŸ§© Enriched Data\", \"ðŸ’¬ Chat with Data\"])\n",
        "\n",
        "# Tab 1: Dashboard\n",
        "with tab1:\n",
        "    st.subheader(\"Company Records\")\n",
        "    st.dataframe(filtered_df[[\n",
        "        \"CIN\", \"Company Name\", \"State\", \"Company Status\",\n",
        "        \"Authorized Capital\", \"Paid Up Capital\", \"Principal Business Activity\"\n",
        "    ]].head(20), use_container_width=True)\n",
        "\n",
        "# Tab 2: Change History\n",
        "with tab2:\n",
        "    st.subheader(\"Daily Change Trends\")\n",
        "    change_counts = all_changes.groupby([\"Date\", \"Change_Type\"]).size().unstack(fill_value=0)\n",
        "    st.bar_chart(change_counts)\n",
        "\n",
        "# Tab 3: Enriched Data\n",
        "with tab3:\n",
        "    st.subheader(\"Enriched Company Info (Sample)\")\n",
        "    enriched_sample = enriched_df.head(15)\n",
        "    st.dataframe(enriched_sample, use_container_width=True)\n",
        "\n",
        "# Tab 4: Chatbot\n",
        "with tab4:\n",
        "    st.subheader(\"Ask Questions in Natural Language\")\n",
        "    user_input = st.text_input(\"ðŸ’¬ Type your query below:\")\n",
        "    if user_input:\n",
        "        response = chatbot_response(user_input)\n",
        "        st.markdown(response)\n",
        "\n",
        "# Show latest AI Summary\n",
        "latest_summary = daily_summaries[\"summaries\"][-1][\"summary\"]\n",
        "st.sidebar.markdown(\"### ðŸ“° Latest AI Summary\")\n",
        "st.sidebar.text(latest_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVDcGq6JgWTK",
        "outputId": "74942119-32fc-4888-9b02-3d5ef7c712e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf\n",
        "conf.get_default().auth_token = \"34IJN7cO4lzAz9UnrJudvvIY0NJ_2cXe5jecf1AD3K7itViyH\"  # â† paste your token"
      ],
      "metadata": {
        "id": "JiFqiG6YiBya"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected Colab cell to launch Streamlit + ngrok\n",
        "!pip install streamlit pyngrok -q\n",
        "!streamlit run app.py &>/dev/null &\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Optional: If you HAVE an ngrok authtoken, uncomment and add it:\n",
        "# ngrok.set_auth_token(\"your_token_here\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"ðŸ”— Dashboard URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvVCMYIRjCYw",
        "outputId": "718401e6-c763-494f-96d2-a525abb7eca9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Dashboard URL: NgrokTunnel: \"https://tiffiny-unshunnable-jorge.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://tiffiny-unshunnable-jorge.ngrok-free.dev/"
      ],
      "metadata": {
        "id": "XrM7aSS_eCap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}